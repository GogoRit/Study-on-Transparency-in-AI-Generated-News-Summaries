# -*- coding: utf-8 -*-
"""DSCI601_SummarizationPipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ln2oNJ7v1-6mlzmeseBLs9XYEekjgaB1
"""

from google.colab import drive
drive.mount('/content/drive')

import json

# Define the file path for the JSON file
file_path = '/content/drive/My Drive/capstone_preprocessedData/preprocessed_dataset_train.json'

# Open the JSON file and load its contents
with open(file_path, 'r') as json_file:
    preprocessed_dataset = json.load(json_file)

# Now 'data' contains the contents of the JSON file as a Python object
# You can work with 'data' as needed
print(preprocessed_dataset)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
import numpy as np
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("cnn_dailymail", "3.0.0")

# Define custom transformer for TF-IDF feature extraction
class ImportantWordsExtractor(BaseEstimator, TransformerMixin):
    def __init__(self, max_features=10):
        self.tfidf_vectorizer = TfidfVectorizer(max_features=max_features)

    def fit(self, X, y=None):
        self.tfidf_vectorizer.fit(X)
        return self

    def transform(self, X):
        # Transform articles into TF-IDF features
        tfidf_matrix = self.tfidf_vectorizer.transform(X)
        # Get feature names (important words) from TF-IDF vectorizer
        feature_names = np.array(self.tfidf_vectorizer.get_feature_names_out())
        # Get indices of top TF-IDF features for each article
        top_feature_indices = tfidf_matrix.toarray().argsort(axis=1)[:, ::-1][:, :self.tfidf_vectorizer.max_features]
        # Extract top TF-IDF features (important words) for each article
        important_words = [feature_names[indices] for indices in top_feature_indices]
        return important_words

# Define a pipeline for extracting important words
important_words_pipeline_train = Pipeline([
    ('important_words_extractor', ImportantWordsExtractor())
])

# Fit the pipeline on the training dataset
print("Fitting important words extraction pipeline on the training dataset...")
important_words_pipeline_train.fit(preprocessed_dataset['train']['article'])
print("Pipeline fitting completed.")

# Apply important words extraction pipeline to the train dataset
print("Fitting and transforming training dataset...")
important_words_train = important_words_pipeline_train.fit_transform(preprocessed_dataset['train']['article'])
print("Done.")

sample1 = preprocessed_dataset["train"][1]
print(f"""Article (total length: {len(sample1["article"])}):""")
print(sample1["article"][:500])
print(f'\nSummary (length: {len(sample1["highlights"])}):')
print(sample1["highlights"])

sample2 = preprocessed_dataset["train"][10]
print(f"""Article (total length: {len(sample2["article"])}):""")
print(sample2["article"][:500])
print(f'\nSummary (length: {len(sample2["highlights"])}):')
print(sample2["highlights"])

import random

# Define the size of the subset (e.g., 1000 articles)
subset_size = 1000

# Sample a subset of articles
random.seed(42)  # Set a seed for reproducibility
subset_indices = random.sample(range(len(preprocessed_dataset['train']['article'])), subset_size)
subset_articles = [preprocessed_dataset['train']['article'][i] for i in subset_indices]

# Tokenize each document in the subset
tokenized_data = [doc.split() for doc in subset_articles]

# Create dictionary and corpus
dictionary = corpora.Dictionary(tokenized_data)
corpus = [dictionary.doc2bow(doc) for doc in tokenized_data]

# Train LDA model
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)

# Print topics
pprint.pprint(lda_model.print_topics())

from sklearn.base import BaseEstimator, TransformerMixin

class CustomPromptGenerator(BaseEstimator, TransformerMixin):
    def __init__(self, max_summary_length=50):
        self.max_summary_length = max_summary_length

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        custom_prompts = []
        for article, important_words in X:
            custom_prompt = self.generate_custom_prompt(article, important_words)
            custom_prompts.append(custom_prompt)
        return custom_prompts

    def generate_custom_prompt(self, article, important_words):
        # Define a template sentence structure for the prompt
        prompt_template = "Summarize the article using the following important words: {}. Limit your summary to {} words."

        # Combine the important words into a comma-separated string
        important_words_str = ", ".join(important_words)

        # Construct the custom prompt by inserting the important words and the summary length limit into the template
        custom_prompt = prompt_template.format(important_words_str, self.max_summary_length)

        return custom_prompt

# Create a pipeline for generating custom prompts
custom_prompt_pipeline = Pipeline([
    ('custom_prompt_generator', CustomPromptGenerator())
])

# Apply custom prompt generation pipeline to the preprocessed dataset
custom_prompts_train = custom_prompt_pipeline.transform(zip(preprocessed_dataset['train']['article'], important_words_train))

# Now, custom_prompts_train, custom_prompts_validation, and custom_prompts_test contain the generated custom prompts
custom_prompts_train[4]

!pip install -q -U bitsandbytes
!pip install -q -U accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sklearn.base import BaseEstimator, TransformerMixin
import bitsandbytes as bnb


class LLAMASummarizer(BaseEstimator, TransformerMixin):
    def __init__(self, model_id="NousResearch/Llama-2-7b-hf"):
        self.model_id = model_id
        self.bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        self.model = AutoModelForCausalLM.from_pretrained(self.model_id, quantization_config=self.bnb_config, device_map="auto")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        generated_summaries = []
        for custom_prompt in X:
            generated_summary = self.generate_summary(custom_prompt)
            generated_summaries.append(generated_summary)
        return generated_summaries

    def generate_summary(self, custom_prompt):
        inputs = self.tokenizer(custom_prompt, return_tensors='pt')
        output = self.model.generate(inputs["input_ids"], max_new_tokens=100)[0]
        decoded_output = self.tokenizer.decode(output, skip_special_tokens=True)
        return decoded_output

# Create a pipeline for LLAMA summarization
llama_summarizer_pipeline = Pipeline([
    ('custom_prompt_generator', CustomPromptGenerator()),
    ('llama_summarizer', LLAMASummarizer())
])

# Apply the LLAMA summarization pipeline to the preprocessed dataset
generated_summaries_train = llama_summarizer_pipeline.transform(zip(preprocessed_dataset['train']['article'], important_words_train))

# Now, generated_summaries_train, generated_summaries_validation, and generated_summaries_test contain the summaries generated by LLAMA-2 using the custom prompts

"""TRYING TO RUN LLAMA2"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id =  "NousResearch/Llama-2-7b-hf"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto")

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

"""ZERO SHOT PROMPT TRY"""

index = 2

dialogue = preprocessed_dataset['train']['article'][index]
summary = preprocessed_dataset['train']['highlights'][index]

prompt = f"""
Summarize the following conversation.

### Input:
{dialogue}

### Summary:
"""

inputs = tokenizer(prompt, return_tensors='pt')
output = tokenizer.decode(
    model.generate(
        inputs["input_ids"],
        max_new_tokens=100,
    )[0],
    skip_special_tokens=True
)

dash_line = '-'.join('' for x in range(100))
print(dash_line)
print(f'INPUT PROMPT:\n{prompt}')
print(dash_line)
print(f'BASELINE HUMAN SUMMARY:\n{summary}\n')
print(dash_line)
print(f'MODEL GENERATION - ZERO SHOT:\n{output}')