# -*- coding: utf-8 -*-
"""DSCI-601 preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pz54J8q7H_cT-scms-pZ1veiTn_ht2hq
"""

#pip install transformers
#pip install datasets
#pip install contractions

huggingface_dataset_name = "cnn_dailymail"

from datasets import load_dataset

dataset = load_dataset(huggingface_dataset_name, "3.0.0")
dataset

sample = dataset["train"][1]
print(f"""Article (total length: {len(sample["article"])}):""")
print(sample["article"][:500])
print(f'\nSummary (length: {len(sample["highlights"])}):')
print(sample["highlights"])

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import re
import contractions
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Function for text preprocessing
class TextPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [self.preprocess_text(text) for text in X]

    def preprocess_text(self, text):
        # Convert to lowercase
        text = text.lower()

        # Remove punctuation and digits
        text = text.translate(str.maketrans('', '', string.punctuation + string.digits))

        # Tokenization
        tokens = word_tokenize(text)

        # Remove stop words
        tokens = [word for word in tokens if word not in self.stop_words]

        # Join the tokens back into a string
        preprocessed_text = ' '.join(tokens)

        return preprocessed_text

# Function for regex cleaning
class RegexCleaner(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [self.re_clean(text) for text in X]

    def re_clean(self, text):
        text = re.sub(r'https?:\/\/.*[\r\n]*', ' ', text, flags=re.MULTILINE)
        text = re.sub(r'\<a href', ' ', text)
        text = re.sub(r'&amp;', ' ', text)
        text = re.sub(r'[_\-;%()|+&=*%:#$@\[\]/]', ' ', text)
        text = re.sub(r'<br />', ' ', text)
        text = re.sub(r'\'', ' ', text)
        text = re.sub(r'\n',' ', text)
        text = re.sub(' est ',' ', text)
        text = re.sub(r'[?!]','.', text)
        return text

# Function to expand contractions
class ContractionsExpander(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [self.expand_contractions(text) for text in X]

    def expand_contractions(self, text):
        return contractions.fix(text)

# Function to remove short sentences
class ShortSentencesRemover(BaseEstimator, TransformerMixin):
    def __init__(self, min_length=5):
        self.min_length = min_length

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [self.remove_short_sentences(text) for text in X]

    def remove_short_sentences(self, text):
        sentences = text.split('.')
        cleaned_sentences = [sentence.strip()+'.' for sentence in sentences if len(sentence.split()) >= self.min_length]
        cleaned_text = ' '.join(cleaned_sentences)
        return cleaned_text

# Function to remove specified tags
class TagsRemover(BaseEstimator, TransformerMixin):
    def __init__(self, tags=['cnn', 'est']):
        self.tags = tags

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [self.remove_tags(text) for text in X]

    def remove_tags(self, text):
        for tag in self.tags:
            tag_pos = text.find(tag)
            if tag_pos != -1 and tag_pos < len(text)//10:
                text = text[tag_pos + len(tag):]
        return text

# Combine the transformers into a preprocessing pipeline
preprocessing_pipeline = Pipeline([
    ('regex_cleaner', RegexCleaner()),
    ('contractions_expander', ContractionsExpander()),
    ('text_preprocessor', TextPreprocessor()),
    ('short_sentences_remover', ShortSentencesRemover()),
    ('tags_remover', TagsRemover())
])

# Apply preprocessing pipeline to the dataset
preprocessed_dataset = dataset.map(lambda example: {'article': preprocessing_pipeline.transform([example['article']])[0],
                                                    'highlights': preprocessing_pipeline.transform([example['highlights']])[0]})

preprocessed_dataset["train"][1]

sample1 = preprocessed_dataset["train"][1]
print(f"""Article (total length: {len(sample1["article"])}):""")
print(sample1["article"][:500])
print(f'\nSummary (length: {len(sample1["highlights"])}):')
print(sample1["highlights"])

from google.colab import drive
drive.mount('/content/drive')

import json

# Save each split of the DatasetDict separately
for split in preprocessed_dataset.keys():
    # Convert split to a list of dictionaries
    split_data_list = preprocessed_dataset[split].to_dict()

    # Define the file path for the split
    file_path = f'/content/drive/My Drive/capstone_preprocessedData/preprocessed_dataset_{split}.json'

    # Save split as a JSON file
    with open(file_path, 'w') as json_file:
        json.dump(split_data_list, json_file)

    print(f"Preprocessed {split} dataset saved successfully.")